{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T07:50:26.256163Z","iopub.execute_input":"2025-02-02T07:50:26.256512Z","iopub.status.idle":"2025-02-02T07:50:26.566094Z","shell.execute_reply.started":"2025-02-02T07:50:26.256484Z","shell.execute_reply":"2025-02-02T07:50:26.565200Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport random\nimport datetime\nimport spacy\nfrom nltk.corpus import wordnet\nimport nltk\n\n# Download required NLTK data\nnltk.download('wordnet')\nnltk.download('omw-1.4')\n\nclass MentalHealthDataGenerator:\n    def __init__(self):\n        # Load spaCy for text processing\n        try:\n            self.nlp = spacy.load('en_core_web_sm')\n        except:\n            # If spaCy model is not installed, download it\n            import os\n            os.system('python -m spacy download en_core_web_sm')\n            self.nlp = spacy.load('en_core_web_sm')\n\n        # Mental health related patterns and phrases\n        self.patterns = {\n            'depression': {\n                'idioms': [\n                    \"under a dark cloud\", \"down in the dumps\", \"at rock bottom\",\n                    \"in a black hole\", \"carrying the weight of the world\",\n                    \"fighting inner demons\", \"lost in darkness\"\n                ],\n                'phrases': [\n                    \"I feel so {}\", \"can't seem to {}\", \"everything feels {}\",\n                    \"no energy to {}\", \"struggling to {}\", \"lost interest in {}\"\n                ],\n                'symptoms': [\n                    \"insomnia\", \"oversleeping\", \"loss of appetite\", \"overeating\",\n                    \"constant fatigue\", \"difficulty concentrating\", \"feeling worthless\",\n                    \"guilt\", \"physical pain\", \"suicidal thoughts\"\n                ],\n                'emotions': [\n                    \"hopeless\", \"empty\", \"worthless\", \"guilty\", \"numb\",\n                    \"sad\", \"miserable\", \"exhausted\", \"lonely\", \"defeated\"\n                ]\n            },\n            'anxiety': {\n                'idioms': [\n                    \"on edge\", \"bundle of nerves\", \"butterflies in stomach\",\n                    \"jumping out of skin\", \"mind racing\", \"heart in mouth\",\n                    \"walking on eggshells\"\n                ],\n                'phrases': [\n                    \"can't stop {}\", \"worried about {}\", \"what if {}\",\n                    \"feeling overwhelmed by {}\", \"scared that {}\", \"nervous about {}\"\n                ],\n                'symptoms': [\n                    \"racing heart\", \"sweating\", \"trembling\", \"chest pain\",\n                    \"shortness of breath\", \"dizziness\", \"nausea\", \"panic attacks\",\n                    \"restlessness\", \"muscle tension\"\n                ],\n                'emotions': [\n                    \"anxious\", \"worried\", \"panicked\", \"stressed\", \"overwhelmed\",\n                    \"fearful\", \"nervous\", \"uneasy\", \"tense\", \"restless\"\n                ]\n            },\n            'normal': {\n                'idioms': [\n                    \"on cloud nine\", \"in high spirits\", \"full of beans\",\n                    \"bright eyed and bushy tailed\", \"right as rain\",\n                    \"feeling on top of the world\"\n                ],\n                'phrases': [\n                    \"enjoying {}\", \"grateful for {}\", \"happy about {}\",\n                    \"looking forward to {}\", \"excited about {}\", \"blessed with {}\"\n                ],\n                'emotions': [\n                    \"happy\", \"content\", \"peaceful\", \"energetic\", \"motivated\",\n                    \"grateful\", \"optimistic\", \"relaxed\", \"balanced\", \"satisfied\"\n                ]\n            }\n        }\n\n        # Social media elements\n        self.social_media_elements = {\n            'hashtags': {\n                'depression': ['#depression', '#mentalhealth', '#depressed', '#anxiety',\n                             '#mentalhealthawareness', '#sad', '#depressing'],\n                'anxiety': ['#anxiety', '#panic', '#stress', '#mentalhealth',\n                           '#anxious', '#worried', '#overthinking'],\n                'normal': ['#blessed', '#grateful', '#happy', '#positive',\n                          '#goodvibes', '#motivation', '#peace']\n            },\n            'emojis': {\n                'depression': ['üò¢', 'üòî', 'üò™', 'üíî', 'üòï', 'üòø'],\n                'anxiety': ['üò∞', 'üò®', 'üò±', 'üòñ', 'üò£', 'üò©'],\n                'normal': ['üòä', 'üòÑ', 'üôè', 'üí™', '‚ú®', 'üåü']\n            }\n        }\n\n    def create_post(self, condition):\n        \"\"\"Create a synthetic social media post\"\"\"\n        patterns = self.patterns[condition]\n\n        # Base structure\n        components = []\n\n        # Add idiom\n        if random.random() < 0.3:\n            components.append(random.choice(patterns['idioms']))\n\n        # Add main phrase\n        if 'phrases' in patterns:\n            phrase = random.choice(patterns['phrases'])\n            emotion = random.choice(patterns['emotions'])\n            components.append(phrase.format(emotion))\n\n        # Add symptom for mental health conditions\n        if condition != 'normal' and random.random() < 0.4:\n            components.append(f\"Experiencing {random.choice(patterns['symptoms'])}\")\n\n        base_text = \" \".join(components)\n\n        # Add social media elements\n        if random.random() < 0.4:\n            hashtags = random.sample(\n                self.social_media_elements['hashtags'][condition],\n                k=random.randint(1, 3)\n            )\n            base_text += \" \" + \" \".join(hashtags)\n\n        if random.random() < 0.3:\n            emojis = random.sample(\n                self.social_media_elements['emojis'][condition],\n                k=random.randint(1, 2)\n            )\n            base_text += \" \" + \"\".join(emojis)\n\n        return base_text\n\n    def generate_dataset(self, n_samples=10000):\n        \"\"\"Generate complete dataset\"\"\"\n        data = []\n        current_time = datetime.datetime.now()\n\n        for _ in range(n_samples):\n            condition = random.choice(['depression', 'anxiety', 'normal'])\n            text = self.create_post(condition)\n\n            # Generate timestamp\n            random_days = random.randint(0, 365)\n            timestamp = current_time - datetime.timedelta(days=random_days)\n\n            # Generate engagement metrics\n            base_engagement = np.random.normal(50, 20)\n            engagement_multiplier = random.uniform(0.8, 1.2)\n            likes = max(0, int(base_engagement * engagement_multiplier))\n            shares = max(0, int(likes * random.uniform(0.1, 0.3)))\n\n            # Create entry\n            entry = {\n                'text': text,\n                'condition': condition,\n                'timestamp': timestamp,\n                'likes': likes,\n                'shares': shares,\n                'word_count': len(text.split()),\n                'char_count': len(text),\n                'has_hashtags': '#' in text,\n                'hashtag_count': text.count('#'),\n                'has_emojis': any(emoji in text for emoji in ''.join(\n                    sum(self.social_media_elements['emojis'].values(), [])\n                ))\n            }\n\n            data.append(entry)\n\n        return pd.DataFrame(data)\n\ndef prepare_dataset():\n    \"\"\"Prepare and split dataset\"\"\"\n    generator = MentalHealthDataGenerator()\n    df = generator.generate_dataset()\n\n    # Convert condition to numeric labels\n    condition_map = {'normal': 0, 'anxiety': 1, 'depression': 2}\n    df['label'] = df['condition'].map(condition_map)\n\n    # Add time-based features\n    df['hour'] = df['timestamp'].dt.hour\n    df['day_of_week'] = df['timestamp'].dt.dayofweek\n\n    # Calculate engagement ratio\n    df['engagement_ratio'] = (df['likes'] + df['shares']) / (df['word_count'] + 1)\n\n    # Print dataset statistics\n    print(\"\\nDataset Statistics:\")\n    print(f\"Total samples: {len(df)}\")\n    print(\"\\nClass distribution:\")\n    print(df['condition'].value_counts())\n\n    # Split dataset\n    train_df, temp_df = train_test_split(\n        df,\n        test_size=0.3,\n        random_state=42,\n        stratify=df['label']\n    )\n\n    val_df, test_df = train_test_split(\n        temp_df,\n        test_size=0.5,\n        random_state=42,\n        stratify=temp_df['label']\n    )\n\n    print(\"\\nDataset splits:\")\n    print(f\"Training samples: {len(train_df)}\")\n    print(f\"Validation samples: {len(val_df)}\")\n    print(f\"Test samples: {len(test_df)}\")\n\n    return train_df, val_df, test_df, list(condition_map.keys())\n\nif __name__ == \"__main__\":\n    train_df, val_df, test_df, classes = prepare_dataset()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T07:50:31.596528Z","iopub.execute_input":"2025-02-02T07:50:31.596952Z","iopub.status.idle":"2025-02-02T07:50:39.706311Z","shell.execute_reply.started":"2025-02-02T07:50:31.596923Z","shell.execute_reply":"2025-02-02T07:50:39.705583Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n\nDataset Statistics:\nTotal samples: 10000\n\nClass distribution:\ncondition\nanxiety       3366\ndepression    3348\nnormal        3286\nName: count, dtype: int64\n\nDataset splits:\nTraining samples: 7000\nValidation samples: 1500\nTest samples: 1500\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import DistilBertTokenizer, DistilBertModel\nimport numpy as np\nfrom tqdm import tqdm\nimport warnings\nimport math\nwarnings.filterwarnings('ignore')\n\nclass MentalHealthDataset(Dataset):\n    def __init__(self, df, tokenizer, max_length=128):\n        self.texts = df['text'].values\n        self.labels = df['label'].values\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n        # Prepare numerical features\n        numerical_features = [\n            'likes', 'shares', 'word_count', 'char_count',\n            'hashtag_count', 'hour', 'day_of_week', 'engagement_ratio',\n            'has_hashtags', 'has_emojis'\n        ]\n\n        # Normalize numerical features safely\n        features = df[numerical_features].values\n        features = features.astype(np.float32)  # Convert to float32\n        mean = np.nanmean(features, axis=0)\n        std = np.nanstd(features, axis=0)\n        std[std == 0] = 1  # Prevent division by zero\n        self.features = (features - mean) / std\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n\n        encoding = self.tokenizer(\n            text,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'features': torch.tensor(self.features[idx], dtype=torch.float32),\n            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n        }\n\nclass MentalHealthClassifier(nn.Module):\n    def __init__(self, n_classes=3, dropout_rate=0.3):\n        super().__init__()\n        self.bert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n\n        # Freeze BERT layers except last 2\n        for param in self.bert.parameters():\n            param.requires_grad = False\n\n        for layer in self.bert.transformer.layer[-2:]:\n            for param in layer.parameters():\n                param.requires_grad = True\n\n        self.text_features = nn.Sequential(\n            nn.Linear(768, 256),\n            nn.LayerNorm(256),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(256, 128),\n            nn.LayerNorm(128),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate)\n        )\n\n        self.numerical_features = nn.Sequential(\n            nn.Linear(10, 32),\n            nn.LayerNorm(32),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate/2),\n            nn.Linear(32, 16),\n            nn.LayerNorm(16),\n            nn.ReLU()\n        )\n\n        self.classifier = nn.Sequential(\n            nn.Linear(144, 64),  # 128 + 16 = 144\n            nn.LayerNorm(64),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(64, n_classes)\n        )\n\n    def forward(self, input_ids, attention_mask, features):\n        # Process text through BERT\n        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        text_features = self.text_features(bert_output.last_hidden_state[:, 0])\n\n        # Process numerical features\n        numerical_output = self.numerical_features(features)\n\n        # Combine features\n        combined = torch.cat([text_features, numerical_output], dim=1)\n\n        return self.classifier(combined)\n\ndef train_epoch(model, train_loader, criterion, optimizer, device):\n    model.train()\n    total_loss = 0\n    correct = 0\n    total = 0\n\n    progress_bar = tqdm(train_loader, desc='Training')\n\n    for batch in progress_bar:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        features = batch['features'].to(device)\n        labels = batch['labels'].to(device)\n\n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask, features)\n        loss = criterion(outputs, labels)\n\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n\n        total_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += labels.size(0)\n        correct += predicted.eq(labels).sum().item()\n\n        progress_bar.set_postfix({\n            'loss': total_loss / (progress_bar.n + 1),\n            'acc': 100. * correct / total\n        })\n\n    return total_loss / len(train_loader), 100. * correct / total\n\ndef evaluate(model, data_loader, criterion, device):\n    model.eval()\n    total_loss = 0\n    correct = 0\n    total = 0\n    predictions = []\n    true_labels = []\n\n    with torch.no_grad():\n        for batch in data_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            features = batch['features'].to(device)\n            labels = batch['labels'].to(device)\n\n            outputs = model(input_ids, attention_mask, features)\n            loss = criterion(outputs, labels)\n\n            total_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n\n            predictions.extend(predicted.cpu().numpy())\n            true_labels.extend(labels.cpu().numpy())\n\n    accuracy = 100. * correct / total\n    avg_loss = total_loss / len(data_loader)\n\n    return {\n        'loss': avg_loss,\n        'accuracy': accuracy,\n        'predictions': predictions,\n        'true_labels': true_labels\n    }\n\ndef train_model(train_df, val_df, test_df, classes, num_epochs=10):\n    # Set device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n\n    # Initialize tokenizer and model\n    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n    model = MentalHealthClassifier(len(classes)).to(device)\n\n    # Create datasets and dataloaders\n    train_dataset = MentalHealthDataset(train_df, tokenizer)\n    val_dataset = MentalHealthDataset(val_df, tokenizer)\n    test_dataset = MentalHealthDataset(test_df, tokenizer)\n\n    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=16)\n    test_loader = DataLoader(test_dataset, batch_size=16)\n\n    # Initialize loss and optimizer\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.AdamW([\n        {'params': model.bert.parameters(), 'lr': 1e-5},\n        {'params': model.text_features.parameters(), 'lr': 2e-4},\n        {'params': model.numerical_features.parameters(), 'lr': 2e-4},\n        {'params': model.classifier.parameters(), 'lr': 2e-4}\n    ], weight_decay=0.01)\n\n    # Training loop\n    best_val_acc = 0\n    patience = 3\n    patience_counter = 0\n    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n\n    try:\n        for epoch in range(num_epochs):\n            print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n\n            # Train\n            train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n\n            # Validate\n            val_metrics = evaluate(model, val_loader, criterion, device)\n\n            # Store metrics\n            history['train_loss'].append(train_loss)\n            history['train_acc'].append(train_acc)\n            history['val_loss'].append(val_metrics['loss'])\n            history['val_acc'].append(val_metrics['accuracy'])\n\n            print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n            print(f\"Val Loss: {val_metrics['loss']:.4f} | Val Acc: {val_metrics['accuracy']:.2f}%\")\n\n            # Save best model\n            if val_metrics['accuracy'] > best_val_acc:\n                best_val_acc = val_metrics['accuracy']\n                torch.save(model.state_dict(), 'best_mental_health_model.pth')\n                patience_counter = 0\n            else:\n                patience_counter += 1\n\n            if patience_counter >= patience:\n                print(\"Early stopping triggered\")\n                break\n\n    except Exception as e:\n        print(f\"Error during training: {str(e)}\")\n        raise\n\n    # Load best model and evaluate on test set\n    model.load_state_dict(torch.load('best_mental_health_model.pth'))\n    test_metrics = evaluate(model, test_loader, criterion, device)\n\n    print(\"\\nTest Results:\")\n    print(f\"Test Loss: {test_metrics['loss']:.4f}\")\n    print(f\"Test Accuracy: {test_metrics['accuracy']:.2f}%\")\n\n    return model, history, test_metrics\n\nif __name__ == \"__main__\":\n    # Get data from Part 1\n    train_df, val_df, test_df, classes = prepare_dataset()\n\n    # Train model\n    model, history, test_metrics = train_model(train_df, val_df, test_df, classes)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T07:50:45.046721Z","iopub.execute_input":"2025-02-02T07:50:45.047206Z","iopub.status.idle":"2025-02-02T07:53:04.172471Z","shell.execute_reply.started":"2025-02-02T07:50:45.047180Z","shell.execute_reply":"2025-02-02T07:53:04.171608Z"}},"outputs":[{"name":"stdout","text":"\nDataset Statistics:\nTotal samples: 10000\n\nClass distribution:\ncondition\nanxiety       3404\nnormal        3317\ndepression    3279\nName: count, dtype: int64\n\nDataset splits:\nTraining samples: 7000\nValidation samples: 1500\nTest samples: 1500\nUsing device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41b2428aafdf4c47860755e63fb12cc0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c38f72d97528491aa5305ee03249e2a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19cbd7fe0aa6481bbffe558b4a3390af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aede7171250f435092341943866e37db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7dd7e92d4d3489d8f79d9dee9fdc5b9"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 438/438 [00:26<00:00, 16.58it/s, loss=0.173, acc=94.6]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1730 | Train Acc: 94.63%\nVal Loss: 0.0060 | Val Acc: 100.00%\n\nEpoch 2/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 438/438 [00:25<00:00, 17.17it/s, loss=0.00789, acc=100]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0079 | Train Acc: 100.00%\nVal Loss: 0.0015 | Val Acc: 100.00%\n\nEpoch 3/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 438/438 [00:25<00:00, 17.24it/s, loss=0.00302, acc=100]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0030 | Train Acc: 100.00%\nVal Loss: 0.0006 | Val Acc: 100.00%\n\nEpoch 4/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 438/438 [00:25<00:00, 17.22it/s, loss=0.00164, acc=100]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0016 | Train Acc: 100.00%\nVal Loss: 0.0003 | Val Acc: 100.00%\nEarly stopping triggered\n\nTest Results:\nTest Loss: 0.0060\nTest Accuracy: 100.00%\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\nimport pandas as pd\nfrom typing import Dict, List, Tuple, Union\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport json\nimport logging\nfrom datetime import datetime\n\nclass MentalHealthPredictor:\n    def __init__(self, model_path: str = 'best_mental_health_model.pth', device=None):\n        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n        self.model = MentalHealthClassifier().to(self.device)\n        self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n        self.model.eval()\n        self.classes = ['normal', 'anxiety', 'depression']\n\n        logging.basicConfig(\n            filename=f'mental_health_predictions_{datetime.now().strftime(\"%Y%m%d\")}.log',\n            level=logging.INFO,\n            format='%(asctime)s - %(levelname)s - %(message)s'\n        )\n        self.logger = logging.getLogger(__name__)\n\n    def preprocess_text(self, text: str) -> Dict[str, torch.Tensor]:\n        \"\"\"Preprocess input text for prediction\"\"\"\n        try:\n            # Tokenize text\n            encoding = self.tokenizer(\n                text,\n                truncation=True,\n                padding='max_length',\n                max_length=128,\n                return_tensors='pt'\n            )\n\n            # Create dummy numerical features with correct dimensions\n            features = torch.zeros((1, 10), dtype=torch.float32)  # Changed to 2D tensor\n\n            return {\n                'input_ids': encoding['input_ids'].to(self.device),\n                'attention_mask': encoding['attention_mask'].to(self.device),\n                'features': features.to(self.device)\n            }\n        except Exception as e:\n            self.logger.error(f\"Error in preprocessing text: {str(e)}\")\n            raise\n\n    def predict(self, text: str) -> Dict:\n        \"\"\"Make prediction for input text\"\"\"\n        try:\n            self.logger.info(f\"Processing text: {text}\")\n\n            inputs = self.preprocess_text(text)\n\n            with torch.no_grad():\n                outputs = self.model(**inputs)\n                probabilities = torch.softmax(outputs, dim=1)\n                prediction = torch.argmax(probabilities, dim=1)\n\n            predicted_class = self.classes[prediction.item()]\n            probabilities = probabilities.squeeze().cpu().numpy()\n\n            result = {\n                'text': text,\n                'prediction': predicted_class,\n                'confidence': float(probabilities[prediction.item()]),\n                'probabilities': {\n                    class_name: float(prob)\n                    for class_name, prob in zip(self.classes, probabilities)\n                }\n            }\n\n            self.logger.info(f\"Prediction result: {json.dumps(result, indent=2)}\")\n            return result\n\n        except Exception as e:\n            self.logger.error(f\"Error in prediction: {str(e)}\")\n            raise\n\nclass ModelEvaluator:\n    def __init__(self, model, test_loader, device, classes):\n        self.model = model\n        self.test_loader = test_loader\n        self.device = device\n        self.classes = classes\n\n    def evaluate_model(self) -> Dict:\n        \"\"\"Perform comprehensive model evaluation\"\"\"\n        self.model.eval()\n        predictions = []\n        true_labels = []\n        probabilities = []\n\n        with torch.no_grad():\n            for batch in self.test_loader:\n                input_ids = batch['input_ids'].to(self.device)\n                attention_mask = batch['attention_mask'].to(self.device)\n                features = batch['features'].to(self.device)\n                labels = batch['labels']\n\n                outputs = self.model(input_ids, attention_mask, features)\n                probs = torch.softmax(outputs, dim=1)\n                _, preds = outputs.max(1)\n\n                predictions.extend(preds.cpu().numpy())\n                true_labels.extend(labels.numpy())\n                probabilities.extend(probs.cpu().numpy())\n\n        return self.calculate_metrics(predictions, true_labels, probabilities)\n\n    def calculate_metrics(self, predictions: List, true_labels: List,\n                         probabilities: List) -> Dict:\n        \"\"\"Calculate various evaluation metrics\"\"\"\n        report = classification_report(true_labels, predictions,\n                                    target_names=self.classes, output_dict=True)\n        cm = confusion_matrix(true_labels, predictions)\n\n        roc_curves = {}\n        for i, class_name in enumerate(self.classes):\n            fpr, tpr, _ = roc_curve(\n                [1 if label == i else 0 for label in true_labels],\n                [prob[i] for prob in probabilities]\n            )\n            roc_curves[class_name] = {\n                'fpr': fpr.tolist(),\n                'tpr': tpr.tolist(),\n                'auc': auc(fpr, tpr)\n            }\n\n        return {\n            'classification_report': report,\n            'confusion_matrix': cm,\n            'roc_curves': roc_curves\n        }\n\nclass ResultVisualizer:\n    @staticmethod\n    def plot_confusion_matrix(cm: np.ndarray, classes: List[str]):\n        \"\"\"Plot confusion matrix using plotly\"\"\"\n        fig = go.Figure(data=go.Heatmap(\n            z=cm,\n            x=classes,\n            y=classes,\n            colorscale='Viridis',\n            text=cm,\n            texttemplate=\"%{text}\",\n            textfont={\"size\": 16},\n            hoverongaps=False))\n\n        fig.update_layout(\n            title='Confusion Matrix',\n            xaxis_title='Predicted Label',\n            yaxis_title='True Label',\n            width=600,\n            height=500\n        )\n\n        return fig\n\n    @staticmethod\n    def plot_roc_curves(roc_curves: Dict):\n        \"\"\"Plot ROC curves for all classes\"\"\"\n        fig = go.Figure()\n\n        for class_name, curve in roc_curves.items():\n            fig.add_trace(go.Scatter(\n                x=curve['fpr'],\n                y=curve['tpr'],\n                name=f'{class_name} (AUC = {curve[\"auc\"]:.3f})',\n                mode='lines'\n            ))\n\n        fig.add_trace(go.Scatter(\n            x=[0, 1],\n            y=[0, 1],\n            name='Random',\n            mode='lines',\n            line=dict(dash='dash', color='gray')\n        ))\n\n        fig.update_layout(\n            title='ROC Curves',\n            xaxis_title='False Positive Rate',\n            yaxis_title='True Positive Rate',\n            width=700,\n            height=500,\n            showlegend=True\n        )\n\n        return fig\n\n    @staticmethod\n    def plot_prediction_distribution(predictions: List[Dict]):\n        \"\"\"Plot distribution of predictions\"\"\"\n        pred_classes = [p['prediction'] for p in predictions]\n        confidences = [p['confidence'] for p in predictions]\n\n        fig = go.Figure()\n\n        # Add histogram for predictions\n        fig.add_trace(go.Histogram(\n            x=pred_classes,\n            name='Predictions'\n        ))\n\n        # Add box plot for confidences\n        fig.add_trace(go.Box(\n            y=confidences,\n            x=pred_classes,\n            name='Confidence'\n        ))\n\n        fig.update_layout(\n            title='Prediction Distribution and Confidence',\n            xaxis_title='Predicted Class',\n            yaxis_title='Count / Confidence',\n            width=800,\n            height=500\n        )\n\n        return fig\n\ndef batch_predict(texts: List[str], predictor: MentalHealthPredictor) -> List[Dict]:\n    \"\"\"Perform batch prediction on multiple texts\"\"\"\n    results = []\n    for text in texts:\n        try:\n            result = predictor.predict(text)\n            results.append(result)\n            print(f\"\\nText: {text}\")\n            print(f\"Predicted Condition: {result['prediction']}\")\n            print(f\"Confidence: {result['confidence']:.2%}\")\n            print(\"\\nProbabilities:\")\n            for condition, prob in result['probabilities'].items():\n                print(f\"{condition}: {prob:.2%}\")\n            print(\"-\" * 80)\n        except Exception as e:\n            print(f\"Error processing text: {text}\")\n            print(f\"Error message: {str(e)}\")\n    return results\n\ndef main():\n    try:\n        # Initialize predictor\n        predictor = MentalHealthPredictor()\n\n        # Example texts for testing\n        test_texts = [\n            \"Feeling really overwhelmed with work and can't stop worrying about deadlines #stress\",\n            \"i am going to temple ‚ú®\",\n            \"Nothing matters anymore. Can't remember the last time I felt happy...\",\n            \"Anxiety is through the roof today. Heart won't stop racing üò∞\",\n            \"Making progress on my goals and feeling positive about the future!\"\n        ]\n\n        # Make predictions\n        results = batch_predict(test_texts, predictor)\n\n        # Create visualizations\n        if results:\n            visualizer = ResultVisualizer()\n            dist_fig = visualizer.plot_prediction_distribution(results)\n            dist_fig.show()\n\n    except Exception as e:\n        print(f\"An error occurred in main: {str(e)}\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T07:53:10.125758Z","iopub.execute_input":"2025-02-02T07:53:10.126426Z","iopub.status.idle":"2025-02-02T07:53:12.227945Z","shell.execute_reply.started":"2025-02-02T07:53:10.126396Z","shell.execute_reply":"2025-02-02T07:53:12.227292Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"\nText: Feeling really overwhelmed with work and can't stop worrying about deadlines #stress\nPredicted Condition: anxiety\nConfidence: 99.21%\n\nProbabilities:\nnormal: 0.28%\nanxiety: 99.21%\ndepression: 0.51%\n--------------------------------------------------------------------------------\n\nText: i am going to temple ‚ú®\nPredicted Condition: normal\nConfidence: 99.22%\n\nProbabilities:\nnormal: 99.22%\nanxiety: 0.32%\ndepression: 0.46%\n--------------------------------------------------------------------------------\n\nText: Nothing matters anymore. Can't remember the last time I felt happy...\nPredicted Condition: depression\nConfidence: 99.34%\n\nProbabilities:\nnormal: 0.36%\nanxiety: 0.31%\ndepression: 99.34%\n--------------------------------------------------------------------------------\n\nText: Anxiety is through the roof today. Heart won't stop racing üò∞\nPredicted Condition: anxiety\nConfidence: 99.05%\n\nProbabilities:\nnormal: 0.29%\nanxiety: 99.05%\ndepression: 0.66%\n--------------------------------------------------------------------------------\n\nText: Making progress on my goals and feeling positive about the future!\nPredicted Condition: normal\nConfidence: 99.50%\n\nProbabilities:\nnormal: 99.50%\nanxiety: 0.28%\ndepression: 0.22%\n--------------------------------------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/html":"<html>\n<head><meta charset=\"utf-8\" /></head>\n<body>\n    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"5253414a-7996-4603-9a5f-764841891f9d\" class=\"plotly-graph-div\" style=\"height:500px; width:800px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"5253414a-7996-4603-9a5f-764841891f9d\")) {                    Plotly.newPlot(                        \"5253414a-7996-4603-9a5f-764841891f9d\",                        [{\"name\":\"Predictions\",\"x\":[\"anxiety\",\"normal\",\"depression\",\"anxiety\",\"normal\"],\"type\":\"histogram\"},{\"name\":\"Confidence\",\"x\":[\"anxiety\",\"normal\",\"depression\",\"anxiety\",\"normal\"],\"y\":[0.9921225905418396,0.9921791553497314,0.993350088596344,0.9904981255531311,0.9949676394462585],\"type\":\"box\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Prediction Distribution and Confidence\"},\"xaxis\":{\"title\":{\"text\":\"Predicted Class\"}},\"yaxis\":{\"title\":{\"text\":\"Count \\u002f Confidence\"}},\"width\":800,\"height\":500},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('5253414a-7996-4603-9a5f-764841891f9d');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                            </script>        </div>\n</body>\n</html>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"!pip install gradio transformers torch\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T07:53:46.206314Z","iopub.execute_input":"2025-02-02T07:53:46.206604Z","iopub.status.idle":"2025-02-02T07:53:56.650304Z","shell.execute_reply.started":"2025-02-02T07:53:46.206583Z","shell.execute_reply":"2025-02-02T07:53:56.649155Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting gradio\n  Downloading gradio-5.14.0-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (22.1.0)\nRequirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\nCollecting fastapi<1.0,>=0.115.2 (from gradio)\n  Downloading fastapi-0.115.8-py3-none-any.whl.metadata (27 kB)\nCollecting ffmpy (from gradio)\n  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\nCollecting gradio-client==1.7.0 (from gradio)\n  Downloading gradio_client-1.7.0-py3-none-any.whl.metadata (7.1 kB)\nRequirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.28.1)\nRequirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.0)\nRequirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\nCollecting markupsafe~=2.0 (from gradio)\n  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\nRequirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\nRequirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.12)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.2)\nRequirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\nRequirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.0.0)\nRequirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.3)\nRequirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\nCollecting python-multipart>=0.0.18 (from gradio)\n  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\nRequirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\nCollecting ruff>=0.9.3 (from gradio)\n  Downloading ruff-0.9.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\nCollecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\nCollecting semantic-version~=2.0 (from gradio)\n  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\nCollecting starlette<1.0,>=0.40.0 (from gradio)\n  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\nCollecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.15.1)\nRequirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\nCollecting uvicorn>=0.14.0 (from gradio)\n  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.7.0->gradio) (2024.9.0)\nRequirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.7.0->gradio) (14.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.12.14)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.0->gradio) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.0->gradio) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.0->gradio) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.0->gradio) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.0->gradio) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.0->gradio) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\nRequirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.27.1)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0,>=1.0->gradio) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0,>=1.0->gradio) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.0->gradio) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<3.0,>=1.0->gradio) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<3.0,>=1.0->gradio) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\nDownloading gradio-5.14.0-py3-none-any.whl (57.7 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m57.7/57.7 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading gradio_client-1.7.0-py3-none-any.whl (321 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m321.9/321.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading fastapi-0.115.8-py3-none-any.whl (94 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\nDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\nDownloading ruff-0.9.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m111.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\nDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\nDownloading starlette-0.45.3-py3-none-any.whl (71 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\nDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\nInstalling collected packages: uvicorn, tomlkit, semantic-version, ruff, python-multipart, markupsafe, ffmpy, starlette, safehttpx, gradio-client, fastapi, gradio\n  Attempting uninstall: markupsafe\n    Found existing installation: MarkupSafe 3.0.2\n    Uninstalling MarkupSafe-3.0.2:\n      Successfully uninstalled MarkupSafe-3.0.2\nSuccessfully installed fastapi-0.115.8 ffmpy-0.5.0 gradio-5.14.0 gradio-client-1.7.0 markupsafe-2.1.5 python-multipart-0.0.20 ruff-0.9.4 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.45.3 tomlkit-0.13.2 uvicorn-0.34.0\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import torch\nfrom transformers import DistilBertTokenizer\nimport gradio as gr\nimport numpy as np\nfrom datetime import datetime\nimport logging\nimport json\n\nclass MentalHealthPredictorInteractive:\n    def __init__(self, model_path: str = 'best_mental_health_model.pth'):\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n        self.model = MentalHealthClassifier().to(self.device)\n        self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n        self.model.eval()\n        self.classes = ['normal', 'anxiety', 'depression']\n\n        # Setup logging\n        logging.basicConfig(\n            filename=f'predictions_{datetime.now().strftime(\"%Y%m%d\")}.log',\n            level=logging.INFO,\n            format='%(asctime)s - %(levelname)s - %(message)s'\n        )\n        self.logger = logging.getLogger(__name__)\n\n    def predict_text(self, text: str) -> dict:\n        \"\"\"Predict mental health condition from input text\"\"\"\n        try:\n            # Tokenize text\n            encoding = self.tokenizer(\n                text,\n                truncation=True,\n                padding='max_length',\n                max_length=128,\n                return_tensors='pt'\n            )\n\n            # Prepare features\n            features = torch.zeros((1, 10), dtype=torch.float32)\n\n            # Move to device\n            input_ids = encoding['input_ids'].to(self.device)\n            attention_mask = encoding['attention_mask'].to(self.device)\n            features = features.to(self.device)\n\n            # Make prediction\n            with torch.no_grad():\n                outputs = self.model(input_ids, attention_mask, features)\n                probabilities = torch.softmax(outputs, dim=1)\n                prediction = torch.argmax(probabilities, dim=1)\n\n            # Get results\n            predicted_class = self.classes[prediction.item()]\n            probabilities = probabilities.squeeze().cpu().numpy()\n\n            # Format results\n            result = {\n                'prediction': predicted_class,\n                'confidence': float(probabilities[prediction.item()]),\n                'probabilities': {\n                    class_name: float(prob)\n                    for class_name, prob in zip(self.classes, probabilities)\n                }\n            }\n\n            return result\n\n        except Exception as e:\n            self.logger.error(f\"Error in prediction: {str(e)}\")\n            return {'error': str(e)}\n\ndef create_gradio_interface():\n    \"\"\"Create Gradio interface for interactive predictions\"\"\"\n    predictor = MentalHealthPredictorInteractive()\n\n    def predict_and_format(text):\n        result = predictor.predict_text(text)\n\n        if 'error' in result:\n            return f\"Error: {result['error']}\"\n\n        # Format output\n        output = f\"Predicted Condition: {result['prediction']}\\n\"\n        output += f\"Confidence: {result['confidence']:.2%}\\n\\n\"\n        output += \"Probabilities:\\n\"\n        for condition, prob in result['probabilities'].items():\n            output += f\"{condition}: {prob:.2%}\\n\"\n\n        return output\n\n    # Create Gradio interface\n    iface = gr.Interface(\n        fn=predict_and_format,\n        inputs=gr.Textbox(\n            lines=3,\n            placeholder=\"Enter your text here...\"\n        ),\n        outputs=gr.Textbox(),\n        title=\"Mental Health Text Analysis\",\n        description=\"Enter text to analyze the emotional content and predict mental health indicators.\",\n        examples=[\n            [\"Feeling really overwhelmed with work and can't stop worrying about deadlines\"],\n            [\"Had a great day at the beach with friends! So grateful for these moments\"],\n            [\"Nothing matters anymore. Can't remember the last time I felt happy\"],\n            [\"Making progress on my goals and feeling positive about the future!\"]\n        ]\n    )\n\n    return iface\n\ndef manual_prediction():\n    \"\"\"Function for manual text input and prediction\"\"\"\n    predictor = MentalHealthPredictorInteractive()\n\n    print(\"\\nMental Health Text Analysis\")\n    print(\"=\" * 50)\n    print(\"Enter 'quit' to exit\")\n    print(\"-\" * 50)\n\n    while True:\n        # Get input\n        text = input(\"\\nEnter text to analyze: \").strip()\n\n        if text.lower() == 'quit':\n            break\n\n        if not text:\n            print(\"Please enter some text to analyze.\")\n            continue\n\n        # Make prediction\n        result = predictor.predict_text(text)\n\n        if 'error' in result:\n            print(f\"\\nError: {result['error']}\")\n            continue\n\n        # Print results\n        print(\"\\nResults:\")\n        print(\"-\" * 20)\n        print(f\"Predicted Condition: {result['prediction']}\")\n        print(f\"Confidence: {result['confidence']:.2%}\")\n        print(\"\\nProbabilities:\")\n        for condition, prob in result['probabilities'].items():\n            print(f\"{condition}: {prob:.2%}\")\n\n        print(\"\\n\" + \"=\" * 50)\n\nif __name__ == \"__main__\":\n    print(\"Choose interface mode:\")\n    print(\"1. Command Line Interface\")\n    print(\"2. Web Interface (Gradio)\")\n\n    choice = input(\"Enter your choice (1 or 2): \").strip()\n\n    if choice == \"1\":\n        manual_prediction()\n    elif choice == \"2\":\n        iface = create_gradio_interface()\n        iface.launch()\n    else:\n        print(\"Invalid choice. Please run again and select 1 or 2.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T07:54:28.608061Z","iopub.execute_input":"2025-02-02T07:54:28.608427Z","iopub.status.idle":"2025-02-02T07:54:39.725520Z","shell.execute_reply.started":"2025-02-02T07:54:28.608395Z","shell.execute_reply":"2025-02-02T07:54:39.724671Z"}},"outputs":[{"name":"stdout","text":"Choose interface mode:\n1. Command Line Interface\n2. Web Interface (Gradio)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your choice (1 or 2):  2\n"},{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7860\nKaggle notebooks require sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\n* Running on public URL: https://412a79c59e6d7c67d8.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://412a79c59e6d7c67d8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"name":"stdout","text":"Created dataset file at: .gradio/flagged/dataset1.csv\n","output_type":"stream"}],"execution_count":6}]}